{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "707dff8d-f7b2-4217-a0e8-08bbe2d8c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7047de05-e54a-4949-9897-28dd700e936c",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f039dc9c-36d3-4d70-b02f-5c6f8214cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\" Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "        \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "        return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def mse(y_pred, y):\n",
    "    \"\"\"Mean Squared Error (MSE) cost function.\"\"\"\n",
    "    return np.mean((y_pred - y) ** 2)\n",
    "\n",
    "def mse_prime(y_pred, y):\n",
    "    \"\"\"Derivative of the Mean Squared Error (MSE) cost function.\"\"\"\n",
    "    return 2 * (y_pred - y) / y.shape[1]\n",
    "    \n",
    "def softmax(z):\n",
    "    \"\"\"Softmax function: Converts input values into probabilities\"\"\" \n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "def softmax_prime(z):\n",
    "    \"\"\"Derivative of softmax\"\"\"\n",
    "    s = softmax(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_prime(z):\n",
    "    \"\"\"Derivative of the ReLU function.\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def leaky_relu(z, a=0.01):\n",
    "    \"\"\"Leaky ReLU activation function.\"\"\"\n",
    "    return np.maximum(a * z, z)\n",
    "\n",
    "def leaky_relu_prime(z, a=0.01):\n",
    "    \"\"\"Derivative of the Leaky ReLU function.\"\"\"\n",
    "    dz = np.ones_like(z)\n",
    "    dz[z <= 0] = a\n",
    "    return dz\n",
    "\n",
    "def cross_entropy(y_pred, y):\n",
    "    \"\"\"Cross-entropy loss function.\"\"\"\n",
    "    m = y.shape[1]\n",
    "    return -np.sum(y * np.log(y_pred + 1e-9)) / m\n",
    "\n",
    "def cross_entropy_prime(y_pred, y):\n",
    "    \"\"\"Derivative of the cross-entropy loss with respect to the softmax input.\"\"\"\n",
    "    return y_pred - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d3916a55-0766-4a8d-b85a-b542ed6a9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, input_size, layer_size, activation, init=None):\n",
    "        self.input_size = input_size\n",
    "        self.layer_size = layer_size\n",
    "        self.a_func = activation # Activation function. Tuple of (function, derivative)\n",
    "\n",
    "        if init == \"he\":\n",
    "            self.he_init()\n",
    "        else:\n",
    "            self.weights = np.random.randn(layer_size, input_size)\n",
    "            self.biases = np.random.randn(layer_size, 1)\n",
    "\n",
    "    \n",
    "    def he_init(self):\n",
    "        \"\"\"He initialization of weights and biases\"\"\"\n",
    "        std = np.sqrt(2. / self.input_size)\n",
    "        self.weights = np.random.randn(self.layer_size, self.input_size) * std\n",
    "        self.biases = np.zeros((self.layer_size, 1))\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the layer.\"\"\"\n",
    "        self.X = X # Cache input X\n",
    "        \n",
    "        z = self.weights @ X + self.biases  # Weighted input\n",
    "        self.z = z\n",
    "        \n",
    "        a = self.a_func[0](z)\n",
    "        self.a = a\n",
    "        \n",
    "        return a\n",
    "        \n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "            Backward pass through the fully connected/dense layer.\n",
    "            dA: gradient from next layer w.r.t  the post activation of this layer, shape (layer_size)\n",
    "        \"\"\"\n",
    "        dz = dA * self.a_func[1](self.z)\n",
    "        # Make sure the shape is correct\n",
    "        #dz = dz.reshape(-1, 1)\n",
    "        #dz = dz.reshape(self.layer_size, 1)\n",
    "        self.dW = dz @ self.X.T\n",
    "        self.db = dz\n",
    "        \n",
    "        dX = self.weights.T @ dz\n",
    "        return dX\n",
    "        \n",
    "\n",
    "\n",
    "class Conv:\n",
    "    def __init__(self, input_size, activation, kernel_size, num_filters=1, stride=1, padding=0, init=None):\n",
    "        # Ensure input_size is a tuple of (height, width, channels)\n",
    "        h, w, c = (list(input_size) + [1, 1, 1])[:3]\n",
    "        self.input_size = (h, w, c)\n",
    "\n",
    "        self.a_func = activation # Activation function. Tuple of (function, derivative)\n",
    "        self.kernel_size = (*kernel_size, self.input_size[2]) # Input kernel size should be a tuple (height, width)\n",
    "        self.num_filters = num_filters\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        if init == \"he\":\n",
    "            self.he_init()\n",
    "        else:\n",
    "            self.weights = np.random.randn(num_filters, *self.kernel_size)\n",
    "            self.biases = np.random.randn(num_filters, 1)\n",
    "\n",
    "\n",
    "    def he_init(self):\n",
    "        \"\"\"He initialization of weights and biases\"\"\"\n",
    "        k_h, k_w, c = self.kernel_size\n",
    "        fan_in = k_h * k_w * c\n",
    "        std = np.sqrt(2. / fan_in)\n",
    "        self.weights = np.random.randn(self.num_filters, k_h, k_w, c) * std\n",
    "        self.biases = np.zeros((self.num_filters, 1))\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the convolutional layer.\"\"\"\n",
    "        self.X = X # Cache input X\n",
    "        K_h, K_w, _ = self.kernel_size # Kernel width and height\n",
    "        stride = self.stride\n",
    "        # Output width of convolution is (W−F+2P)/S+1 where W is width of input F is width of filter(kernel), P is the padding, and S is the stride. Same for height.\n",
    "        # Here we assume no padding P=0.\n",
    "        H_out = (X.shape[0] - K_h) // stride + 1\n",
    "        W_out = (X.shape[1] - K_w) // stride + 1\n",
    "        # Output size of the convolutional layer is (output_height, output_width, num_filters)\n",
    "        self.a = np.zeros((H_out, W_out, self.num_filters))  # List to store activations.\n",
    "        self.z = np.zeros((W_out, W_out, self.num_filters))  # List to store pre-activations\n",
    "\n",
    "        W, b = self.weights, self.biases\n",
    "\n",
    "        # Perform convolution\n",
    "        for f in range(self.num_filters):\n",
    "            for i in range(H_out):\n",
    "                for j in range(W_out):\n",
    "                    # Calculate the starting index for the region of interest\n",
    "                    i0 = i * stride\n",
    "                    j0 = j * stride\n",
    "                    # Calculate the ending index for the region of interest\n",
    "                    i1 = i0 + K_h\n",
    "                    j1 = j0 + K_w\n",
    "                    # Extract the region of interest\n",
    "                    region = X[i0:i1, j0:j1, :] # Note: region is of shape (K_h, K_w, input_channels)\n",
    "                    # Perform convolution operation and store the pre-activations and activations\n",
    "                    z = np.sum(region * W[f]) + b[f]\n",
    "                    self.z[i, j, f] = z.item()\n",
    "                    # Apply activation function\n",
    "                    a = self.a_func[0](z)\n",
    "                    self.a[i, j, f] = a.item()\n",
    "\n",
    "        return self.a\n",
    "    \n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        dA: ∂L/∂a, shape (H_out, W_out, F)\n",
    "        returns dX: ∂L/∂X, shape (H, W, C)\n",
    "        \"\"\"\n",
    "        X = self.X  # (H, W, C)\n",
    "        W, b = self.weights, self.biases\n",
    "        K_h, K_w, _ = self.kernel_size\n",
    "        stride = self.stride\n",
    "        H_in, W_in, C = X.shape\n",
    "        H_out, W_out, F = dA.shape\n",
    "\n",
    "        dZ = dA * self.a_func[1](self.z)\n",
    "\n",
    "        # backprop through activation: dZ = dA * σ'(z)\n",
    "        dZ = dA * self.a_func[1](self.z)  # (H_out, W_out, F)\n",
    "\n",
    "        dW = np.zeros_like(self.weights)  # (F, K_h, K_w, C)\n",
    "        db = np.zeros_like(self.biases)   # (F, 1)\n",
    "        dX = np.zeros_like(X)             # (H, W_in, C)\n",
    "\n",
    "        for f in range(F):\n",
    "            # bias gradient: sum over all positions\n",
    "            db[f, 0] = np.sum(dZ[:, :, f])\n",
    "\n",
    "            for i in range(H_out):\n",
    "                for j in range(W_out):\n",
    "                    i0 = i * stride\n",
    "                    j0 = j * stride\n",
    "\n",
    "                    i1 = i0 + K_h\n",
    "                    j1 = j0 + K_w\n",
    "                    \n",
    "                    region = X[i0:i1, j0:j1, :]  # (K_h, K_w, C)\n",
    "\n",
    "                    grad = dZ[i, j, f] # ∂L/∂z_{i,j,f}\n",
    "                    # weight gradient\n",
    "                    dW[f] += region * grad\n",
    "                    # input gradient\n",
    "                    dX[i0:i1, j0:j1, :] += W[f] * grad\n",
    "\n",
    "        self.dW, self.db = dW, db\n",
    "        return dX\n",
    "\n",
    "\n",
    "\n",
    "class MaxPool:\n",
    "    def __init__(self, input_size, kernel_size, stride=1):\n",
    "        # Ensure input_size is a tuple of (height, width, number of input matricies)\n",
    "        h, w, c = (list(input_size) + [1, 1, 1])[:3]\n",
    "        self.input_size = (h, w, c)\n",
    "\n",
    "        self.kernel_size = (*kernel_size, self.input_size[2]) # Kernel size should be a tuple (height, width)\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the convolutional layer.\"\"\"\n",
    "        # Ensure X is 3D: (H, W, C)\n",
    "        if X.ndim == 2:\n",
    "            # Assume (H, W) dimesions, add channel dimension\n",
    "            X = X[:, :, np.newaxis]\n",
    "        elif X.ndim != 3:\n",
    "            raise ValueError(f\"MaxPool layer expected 3D input (H,W,C), got shape {X.shape}\")\n",
    "        \n",
    "        self.X = X # Cache input X\n",
    "        K_h, K_w, _ = self.kernel_size # Kernel width and height\n",
    "        stride = self.stride\n",
    "        # Output width of convolution is (W−F+2P)/S+1 \n",
    "        # where W is width of input F is width of filter(kernel), P is the padding, and S is the stride. Same for height.\n",
    "        # Here we assume no padding P=0.\n",
    "        H_out = (X.shape[0] - K_h) // stride + 1\n",
    "        W_out = (X.shape[1] - K_w) // stride + 1\n",
    "        # Output size of the convolutional layer is (output_height, output_width, num_filters)\n",
    "        self.a = np.zeros((H_out, W_out, X.shape[2]))  # List to store activations.\n",
    "\n",
    "        self.max_idx = {}  # store argmax positions\n",
    "\n",
    "        for c in range(X.shape[2]):\n",
    "            for i in range(H_out):\n",
    "                for j in range(W_out):\n",
    "                    # Calculate the starting index for the region of interest\n",
    "                    i0 = i * stride\n",
    "                    j0 = j * stride\n",
    "                    # Calculate the ending index for the region of interest\n",
    "                    i1 = i0 + K_h\n",
    "                    j1 = j0 + K_w\n",
    "                    # Extract the region of interest\n",
    "                    region = X[i0:i1, j0:j1, c] # Note: region is of shape (K_h, K_w, input matricies(X.shape[2]))\n",
    "                    # Extract indicies of maximum value in region of interest and store it\n",
    "                    idx = np.unravel_index(np.argmax(region), region.shape)\n",
    "                    self.max_idx[(i,j,c)] = (i0+idx[0], j0+idx[1], c)\n",
    "                    # Extract maximum value from region and store it\n",
    "                    self.a[i, j, c] = region[idx]\n",
    "\n",
    "        return self.a\n",
    "    \n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "            Backward pass through the MaxPool layer. \n",
    "            dA: gradient from next layer, shape (H_out, W_out, C)\n",
    "        \"\"\"\n",
    "        dX = np.zeros_like(self.X)\n",
    "        H_out, W_out = dA.shape[0], dA.shape[1]\n",
    "        \n",
    "        for c in range(self.X.shape[2]):\n",
    "            for i in range(H_out):\n",
    "                for j in range(W_out):\n",
    "                    # route gradient through max position\n",
    "                    # other position's gradient is 0\n",
    "                    pi, pj, pc = self.max_idx[(i,j,c)]\n",
    "                    dX[pi,pj,pc] = dA[i,j,c]\n",
    "\n",
    "        return dX\n",
    "\n",
    "\n",
    "\n",
    "class Flatten:\n",
    "    def forward(self, X):\n",
    "        self.input_shape = X.shape\n",
    "        return X.reshape(-1, 1)\n",
    "    def backward(self, dA):\n",
    "        return dA.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa296df5-d6df-4f9c-8c0a-1a544e3b7fd2",
   "metadata": {},
   "source": [
    "## Neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f8010935-e015-495d-bbcc-aaf53268c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, cost=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            layers (list): List of layer objects (Dense, Conv, MaxPool, etc.).\n",
    "            cost (tuple): (cost_func, cost_prime) for the network.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.cost = cost  # (cost_func, cost_prime)\n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        a = X\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "        self.output = a\n",
    "        return a\n",
    "\n",
    "    \n",
    "    def backprop(self, y):\n",
    "        \"\"\"\n",
    "        Backward pass through the network (assumes last layer is differentiable).\n",
    "        Uses the cost function for loss derivative wrt network output.\n",
    "        \"\"\"\n",
    "        # Compute dA for output layer\n",
    "        cost_prime = self.cost[1]\n",
    "        y = y.reshape(-1,1)\n",
    "        dA = cost_prime(self.output, y)\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "        # Gradients are stored as attributes in each layer for update\n",
    "\n",
    "    \n",
    "    def update_parameters(self, eta, batch_size):\n",
    "        \"\"\"\n",
    "        For layers with weights/biases, update using stored gradients.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, \"weights\") and hasattr(layer, \"dW\"):\n",
    "                layer.weights -= eta * (layer.dW / batch_size)\n",
    "            if hasattr(layer, \"biases\") and hasattr(layer, \"db\"):\n",
    "                layer.biases -= eta * (layer.db / batch_size)\n",
    "                \n",
    "\n",
    "    def train(self, x_train, y_train, epochs, batch_size, eta, results=False, print_every=10):\n",
    "        num_samples = len(x_train)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(num_samples)\n",
    "            X_shuffled = [x_train[i] for i in indices]\n",
    "            y_shuffled = [y_train[i] for i in indices]\n",
    "\n",
    "            if results:\n",
    "                epoch_outputs, epoch_labels = [], []\n",
    "\n",
    "            batch_count = 0\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "                # Zero gradients for all layers before batch\n",
    "                for layer in self.layers:\n",
    "                    if hasattr(layer, \"dW\"):\n",
    "                        layer.dW = np.zeros_like(layer.weights)\n",
    "                    if hasattr(layer, \"db\"):\n",
    "                        layer.db = np.zeros_like(layer.biases)\n",
    "\n",
    "                if results:\n",
    "                    batch_outputs, batch_labels = [], []\n",
    "\n",
    "                \n",
    "                for x, y in zip(X_batch, y_batch):\n",
    "                    out = self.forward(x)\n",
    "                    self.backprop(y)\n",
    "\n",
    "                    if results:\n",
    "                        epoch_outputs.append(out.flatten())\n",
    "                        epoch_labels.append(y.flatten())\n",
    "                    \n",
    "                        batch_outputs.append(out.flatten())\n",
    "                        batch_labels.append(y.flatten())\n",
    "\n",
    "                # Update parameters after batch\n",
    "                self.update_parameters(eta, len(X_batch))\n",
    "                batch_count += 1\n",
    "\n",
    "                # Print batch results at the specified interval\n",
    "                if results and (batch_count % print_every == 0):\n",
    "                    batch_outputs = np.array(batch_outputs)\n",
    "                    batch_labels = np.array(batch_labels)\n",
    "                    batch_loss = self.cost[0](batch_outputs, batch_labels)\n",
    "                    batch_preds = np.argmax(batch_outputs, axis=1)\n",
    "                    batch_true = np.argmax(batch_labels, axis=1)\n",
    "                    batch_acc = np.mean(batch_preds == batch_true)\n",
    "                    print(f\"Epoch {epoch+1}, Batch {batch_count}, Loss: {batch_loss:.4f}, Accuracy: {batch_acc * 100:.2f}%\")\n",
    "\n",
    "            if results and epoch_outputs:\n",
    "                epoch_outputs = np.array(epoch_outputs)\n",
    "                epoch_labels = np.array(epoch_labels)\n",
    "                loss = self.cost[0](epoch_outputs, epoch_labels)\n",
    "                predictions = np.argmax(epoch_outputs, axis=1)\n",
    "                true_labels = np.argmax(epoch_labels, axis=1)\n",
    "                accuracy = np.mean(predictions == true_labels)\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cac0d1-6763-4feb-8724-0e3d7eae42f7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa12506-f9ef-4559-a5e7-1fe11a1ec8ee",
   "metadata": {},
   "source": [
    "### Load MNIST dataset and seperate test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bc624376-83d0-42b2-b2de-ae421fef2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data to the range [0, 1]\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Add channel dimension: (28, 28) -> (28, 28, 1)\n",
    "x_train = x_train[..., np.newaxis]\n",
    "x_test = x_test[..., np.newaxis]\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3610630-a499-452b-8607-9bc33bea1070",
   "metadata": {},
   "source": [
    "### Initialize neural network and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6a456832-b1d1-4af2-ae16-66aa998a2128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10, Loss: 31.7894, Accuracy: 13.28%\n",
      "Epoch 1, Batch 20, Loss: 32.2607, Accuracy: 13.28%\n",
      "Epoch 1, Batch 30, Loss: 33.3363, Accuracy: 9.38%\n",
      "Epoch 1, Batch 40, Loss: 32.0672, Accuracy: 11.72%\n",
      "Epoch 1, Batch 50, Loss: 31.3374, Accuracy: 15.62%\n",
      "Epoch 1, Batch 60, Loss: 30.4711, Accuracy: 17.19%\n",
      "Epoch 1, Batch 70, Loss: 31.0999, Accuracy: 16.41%\n",
      "Epoch 1, Batch 80, Loss: 31.0843, Accuracy: 14.06%\n",
      "Epoch 1, Batch 90, Loss: 30.6520, Accuracy: 15.62%\n",
      "Epoch 1, Batch 100, Loss: 31.7822, Accuracy: 16.41%\n",
      "Epoch 1, Batch 110, Loss: 30.4703, Accuracy: 16.41%\n",
      "Epoch 1, Batch 120, Loss: 30.3300, Accuracy: 17.19%\n",
      "Epoch 1, Batch 130, Loss: 30.1218, Accuracy: 21.09%\n",
      "Epoch 1, Batch 140, Loss: 31.0901, Accuracy: 16.41%\n",
      "Epoch 1, Batch 150, Loss: 30.9981, Accuracy: 17.97%\n",
      "Epoch 1, Batch 160, Loss: 29.3098, Accuracy: 24.22%\n",
      "Epoch 1, Batch 170, Loss: 30.5865, Accuracy: 14.84%\n",
      "Epoch 1, Batch 180, Loss: 29.6017, Accuracy: 19.53%\n",
      "Epoch 1, Batch 190, Loss: 31.1430, Accuracy: 11.72%\n",
      "Epoch 1, Batch 200, Loss: 29.3332, Accuracy: 17.19%\n",
      "Epoch 1, Batch 210, Loss: 30.9046, Accuracy: 11.72%\n",
      "Epoch 1, Batch 220, Loss: 29.6857, Accuracy: 14.06%\n",
      "Epoch 1, Batch 230, Loss: 28.1933, Accuracy: 25.00%\n",
      "Epoch 1, Batch 240, Loss: 30.0098, Accuracy: 14.06%\n",
      "Epoch 1, Batch 250, Loss: 31.6867, Accuracy: 13.28%\n",
      "Epoch 1, Batch 260, Loss: 29.1564, Accuracy: 20.31%\n",
      "Epoch 1, Batch 270, Loss: 28.5875, Accuracy: 19.53%\n",
      "Epoch 1, Batch 280, Loss: 27.6898, Accuracy: 20.31%\n",
      "Epoch 1, Batch 290, Loss: 28.4899, Accuracy: 17.19%\n",
      "Epoch 1, Batch 300, Loss: 30.1369, Accuracy: 20.31%\n",
      "Epoch 1, Batch 310, Loss: 29.7176, Accuracy: 14.84%\n",
      "Epoch 1, Batch 320, Loss: 28.0383, Accuracy: 19.53%\n",
      "Epoch 1, Batch 330, Loss: 28.8809, Accuracy: 17.97%\n",
      "Epoch 1, Batch 340, Loss: 28.9921, Accuracy: 21.09%\n",
      "Epoch 1, Batch 350, Loss: 29.3845, Accuracy: 20.31%\n",
      "Epoch 1, Batch 360, Loss: 28.4117, Accuracy: 22.66%\n",
      "Epoch 1, Batch 370, Loss: 29.4637, Accuracy: 18.75%\n",
      "Epoch 1, Batch 380, Loss: 28.1372, Accuracy: 20.31%\n",
      "Epoch 1, Batch 390, Loss: 29.0501, Accuracy: 17.97%\n",
      "Epoch 1, Batch 400, Loss: 28.6173, Accuracy: 24.22%\n",
      "Epoch 1, Batch 410, Loss: 28.3489, Accuracy: 21.88%\n",
      "Epoch 1, Batch 420, Loss: 29.7290, Accuracy: 16.41%\n",
      "Epoch 1, Batch 430, Loss: 26.9955, Accuracy: 28.91%\n",
      "Epoch 1, Batch 440, Loss: 28.6573, Accuracy: 26.56%\n",
      "Epoch 1, Batch 450, Loss: 27.6692, Accuracy: 32.03%\n",
      "Epoch 1, Batch 460, Loss: 26.1647, Accuracy: 39.84%\n",
      "Epoch 1/5, Loss: 13955.5614, Accuracy: 18.72%\n",
      "Epoch 2, Batch 10, Loss: 27.8245, Accuracy: 34.38%\n",
      "Epoch 2, Batch 20, Loss: 28.4133, Accuracy: 25.78%\n",
      "Epoch 2, Batch 30, Loss: 28.8510, Accuracy: 23.44%\n",
      "Epoch 2, Batch 40, Loss: 28.0322, Accuracy: 26.56%\n",
      "Epoch 2, Batch 50, Loss: 27.2127, Accuracy: 28.91%\n",
      "Epoch 2, Batch 60, Loss: 26.7920, Accuracy: 32.81%\n",
      "Epoch 2, Batch 70, Loss: 27.8934, Accuracy: 27.34%\n",
      "Epoch 2, Batch 80, Loss: 27.5116, Accuracy: 28.12%\n",
      "Epoch 2, Batch 90, Loss: 27.7526, Accuracy: 28.91%\n",
      "Epoch 2, Batch 100, Loss: 28.0413, Accuracy: 25.00%\n",
      "Epoch 2, Batch 110, Loss: 27.5185, Accuracy: 31.25%\n",
      "Epoch 2, Batch 120, Loss: 28.1246, Accuracy: 27.34%\n",
      "Epoch 2, Batch 130, Loss: 28.4895, Accuracy: 26.56%\n",
      "Epoch 2, Batch 140, Loss: 27.5639, Accuracy: 28.12%\n",
      "Epoch 2, Batch 150, Loss: 27.8468, Accuracy: 31.25%\n",
      "Epoch 2, Batch 160, Loss: 27.3132, Accuracy: 27.34%\n",
      "Epoch 2, Batch 170, Loss: 28.3318, Accuracy: 21.09%\n",
      "Epoch 2, Batch 180, Loss: 26.7182, Accuracy: 37.50%\n",
      "Epoch 2, Batch 190, Loss: 27.4950, Accuracy: 28.12%\n",
      "Epoch 2, Batch 200, Loss: 27.6862, Accuracy: 31.25%\n",
      "Epoch 2, Batch 210, Loss: 25.7359, Accuracy: 36.72%\n",
      "Epoch 2, Batch 220, Loss: 27.4076, Accuracy: 32.81%\n",
      "Epoch 2, Batch 230, Loss: 27.6289, Accuracy: 29.69%\n",
      "Epoch 2, Batch 240, Loss: 26.4555, Accuracy: 29.69%\n",
      "Epoch 2, Batch 250, Loss: 27.3910, Accuracy: 28.91%\n",
      "Epoch 2, Batch 260, Loss: 26.4931, Accuracy: 34.38%\n",
      "Epoch 2, Batch 270, Loss: 26.1850, Accuracy: 33.59%\n",
      "Epoch 2, Batch 280, Loss: 27.8965, Accuracy: 21.88%\n",
      "Epoch 2, Batch 290, Loss: 26.1148, Accuracy: 30.47%\n",
      "Epoch 2, Batch 300, Loss: 26.6167, Accuracy: 30.47%\n",
      "Epoch 2, Batch 310, Loss: 27.1571, Accuracy: 27.34%\n",
      "Epoch 2, Batch 320, Loss: 26.1866, Accuracy: 35.16%\n",
      "Epoch 2, Batch 330, Loss: 25.9674, Accuracy: 35.16%\n",
      "Epoch 2, Batch 340, Loss: 27.8136, Accuracy: 32.03%\n",
      "Epoch 2, Batch 350, Loss: 27.7579, Accuracy: 29.69%\n",
      "Epoch 2, Batch 360, Loss: 27.6573, Accuracy: 29.69%\n",
      "Epoch 2, Batch 370, Loss: 26.7482, Accuracy: 29.69%\n",
      "Epoch 2, Batch 380, Loss: 26.0931, Accuracy: 30.47%\n",
      "Epoch 2, Batch 390, Loss: 28.4394, Accuracy: 22.66%\n",
      "Epoch 2, Batch 400, Loss: 25.5834, Accuracy: 31.25%\n",
      "Epoch 2, Batch 410, Loss: 25.4682, Accuracy: 33.59%\n",
      "Epoch 2, Batch 420, Loss: 26.2695, Accuracy: 32.03%\n",
      "Epoch 2, Batch 430, Loss: 27.0400, Accuracy: 26.56%\n",
      "Epoch 2, Batch 440, Loss: 26.4241, Accuracy: 28.91%\n",
      "Epoch 2, Batch 450, Loss: 26.5642, Accuracy: 34.38%\n",
      "Epoch 2, Batch 460, Loss: 25.0767, Accuracy: 39.84%\n",
      "Epoch 2/5, Loss: 12842.2250, Accuracy: 29.40%\n",
      "Epoch 3, Batch 10, Loss: 26.3167, Accuracy: 31.25%\n",
      "Epoch 3, Batch 20, Loss: 26.4093, Accuracy: 34.38%\n",
      "Epoch 3, Batch 30, Loss: 25.2997, Accuracy: 39.84%\n",
      "Epoch 3, Batch 40, Loss: 26.6324, Accuracy: 28.91%\n",
      "Epoch 3, Batch 50, Loss: 24.8756, Accuracy: 37.50%\n",
      "Epoch 3, Batch 60, Loss: 24.0145, Accuracy: 42.19%\n",
      "Epoch 3, Batch 70, Loss: 27.3157, Accuracy: 30.47%\n",
      "Epoch 3, Batch 80, Loss: 28.2052, Accuracy: 24.22%\n",
      "Epoch 3, Batch 90, Loss: 25.2893, Accuracy: 32.81%\n",
      "Epoch 3, Batch 100, Loss: 24.9259, Accuracy: 42.19%\n",
      "Epoch 3, Batch 110, Loss: 26.7952, Accuracy: 33.59%\n",
      "Epoch 3, Batch 120, Loss: 26.5503, Accuracy: 32.81%\n",
      "Epoch 3, Batch 130, Loss: 25.4074, Accuracy: 37.50%\n",
      "Epoch 3, Batch 140, Loss: 25.7426, Accuracy: 38.28%\n",
      "Epoch 3, Batch 150, Loss: 26.6625, Accuracy: 34.38%\n",
      "Epoch 3, Batch 160, Loss: 26.0919, Accuracy: 38.28%\n",
      "Epoch 3, Batch 170, Loss: 27.2970, Accuracy: 34.38%\n",
      "Epoch 3, Batch 180, Loss: 25.1280, Accuracy: 36.72%\n",
      "Epoch 3, Batch 190, Loss: 26.6546, Accuracy: 35.16%\n",
      "Epoch 3, Batch 200, Loss: 25.2064, Accuracy: 33.59%\n",
      "Epoch 3, Batch 210, Loss: 24.1150, Accuracy: 39.84%\n",
      "Epoch 3, Batch 220, Loss: 25.8177, Accuracy: 32.81%\n",
      "Epoch 3, Batch 230, Loss: 25.5945, Accuracy: 35.94%\n",
      "Epoch 3, Batch 240, Loss: 24.8971, Accuracy: 39.06%\n",
      "Epoch 3, Batch 250, Loss: 24.5102, Accuracy: 37.50%\n",
      "Epoch 3, Batch 260, Loss: 25.0125, Accuracy: 36.72%\n",
      "Epoch 3, Batch 270, Loss: 25.1794, Accuracy: 39.84%\n",
      "Epoch 3, Batch 280, Loss: 24.8285, Accuracy: 39.06%\n",
      "Epoch 3, Batch 290, Loss: 26.2224, Accuracy: 30.47%\n",
      "Epoch 3, Batch 300, Loss: 24.3173, Accuracy: 34.38%\n",
      "Epoch 3, Batch 310, Loss: 24.7987, Accuracy: 41.41%\n",
      "Epoch 3, Batch 320, Loss: 23.6219, Accuracy: 39.84%\n",
      "Epoch 3, Batch 330, Loss: 22.8454, Accuracy: 43.75%\n",
      "Epoch 3, Batch 340, Loss: 24.4791, Accuracy: 41.41%\n",
      "Epoch 3, Batch 350, Loss: 24.3595, Accuracy: 42.97%\n",
      "Epoch 3, Batch 360, Loss: 23.9952, Accuracy: 43.75%\n",
      "Epoch 3, Batch 370, Loss: 24.8609, Accuracy: 38.28%\n",
      "Epoch 3, Batch 380, Loss: 22.6550, Accuracy: 50.78%\n",
      "Epoch 3, Batch 390, Loss: 24.3550, Accuracy: 41.41%\n",
      "Epoch 3, Batch 400, Loss: 24.3270, Accuracy: 42.19%\n",
      "Epoch 3, Batch 410, Loss: 23.8257, Accuracy: 39.84%\n",
      "Epoch 3, Batch 420, Loss: 23.5208, Accuracy: 46.88%\n",
      "Epoch 3, Batch 430, Loss: 25.0313, Accuracy: 42.19%\n",
      "Epoch 3, Batch 440, Loss: 23.8320, Accuracy: 41.41%\n",
      "Epoch 3, Batch 450, Loss: 23.1385, Accuracy: 46.09%\n",
      "Epoch 3, Batch 460, Loss: 21.3606, Accuracy: 50.00%\n",
      "Epoch 3/5, Loss: 11669.1609, Accuracy: 38.84%\n",
      "Epoch 4, Batch 10, Loss: 24.3890, Accuracy: 38.28%\n",
      "Epoch 4, Batch 20, Loss: 24.2035, Accuracy: 39.06%\n",
      "Epoch 4, Batch 30, Loss: 22.9458, Accuracy: 44.53%\n",
      "Epoch 4, Batch 40, Loss: 22.2269, Accuracy: 50.78%\n",
      "Epoch 4, Batch 50, Loss: 23.9765, Accuracy: 43.75%\n",
      "Epoch 4, Batch 60, Loss: 24.2653, Accuracy: 45.31%\n",
      "Epoch 4, Batch 70, Loss: 22.7772, Accuracy: 44.53%\n",
      "Epoch 4, Batch 80, Loss: 25.6883, Accuracy: 40.62%\n",
      "Epoch 4, Batch 90, Loss: 24.1866, Accuracy: 44.53%\n",
      "Epoch 4, Batch 100, Loss: 22.7887, Accuracy: 49.22%\n",
      "Epoch 4, Batch 110, Loss: 23.1354, Accuracy: 50.00%\n",
      "Epoch 4, Batch 120, Loss: 24.7049, Accuracy: 42.19%\n",
      "Epoch 4, Batch 130, Loss: 21.1750, Accuracy: 50.78%\n",
      "Epoch 4, Batch 140, Loss: 21.0886, Accuracy: 55.47%\n",
      "Epoch 4, Batch 150, Loss: 23.1633, Accuracy: 49.22%\n",
      "Epoch 4, Batch 160, Loss: 20.4898, Accuracy: 54.69%\n",
      "Epoch 4, Batch 170, Loss: 21.9268, Accuracy: 47.66%\n",
      "Epoch 4, Batch 180, Loss: 23.7549, Accuracy: 48.44%\n",
      "Epoch 4, Batch 190, Loss: 22.2700, Accuracy: 54.69%\n",
      "Epoch 4, Batch 200, Loss: 22.6262, Accuracy: 44.53%\n",
      "Epoch 4, Batch 210, Loss: 21.9369, Accuracy: 47.66%\n",
      "Epoch 4, Batch 220, Loss: 22.0748, Accuracy: 49.22%\n",
      "Epoch 4, Batch 230, Loss: 23.6451, Accuracy: 48.44%\n",
      "Epoch 4, Batch 240, Loss: 22.3110, Accuracy: 50.78%\n",
      "Epoch 4, Batch 250, Loss: 20.5003, Accuracy: 53.91%\n",
      "Epoch 4, Batch 260, Loss: 24.2449, Accuracy: 43.75%\n",
      "Epoch 4, Batch 270, Loss: 20.3230, Accuracy: 56.25%\n",
      "Epoch 4, Batch 280, Loss: 21.9270, Accuracy: 52.34%\n",
      "Epoch 4, Batch 290, Loss: 21.6086, Accuracy: 58.59%\n",
      "Epoch 4, Batch 300, Loss: 22.7043, Accuracy: 43.75%\n",
      "Epoch 4, Batch 310, Loss: 23.3137, Accuracy: 44.53%\n",
      "Epoch 4, Batch 320, Loss: 21.2463, Accuracy: 53.12%\n",
      "Epoch 4, Batch 330, Loss: 23.8737, Accuracy: 42.97%\n",
      "Epoch 4, Batch 340, Loss: 22.3216, Accuracy: 46.88%\n",
      "Epoch 4, Batch 350, Loss: 20.1990, Accuracy: 53.91%\n",
      "Epoch 4, Batch 360, Loss: 19.0113, Accuracy: 61.72%\n",
      "Epoch 4, Batch 370, Loss: 21.2384, Accuracy: 51.56%\n",
      "Epoch 4, Batch 380, Loss: 22.2884, Accuracy: 50.00%\n",
      "Epoch 4, Batch 390, Loss: 19.2467, Accuracy: 53.91%\n",
      "Epoch 4, Batch 400, Loss: 20.1890, Accuracy: 63.28%\n",
      "Epoch 4, Batch 410, Loss: 21.6541, Accuracy: 52.34%\n",
      "Epoch 4, Batch 420, Loss: 21.3314, Accuracy: 50.00%\n",
      "Epoch 4, Batch 430, Loss: 20.9328, Accuracy: 55.47%\n",
      "Epoch 4, Batch 440, Loss: 20.8464, Accuracy: 53.12%\n",
      "Epoch 4, Batch 450, Loss: 21.8610, Accuracy: 53.91%\n",
      "Epoch 4, Batch 460, Loss: 21.6127, Accuracy: 53.12%\n",
      "Epoch 4/5, Loss: 10341.0478, Accuracy: 49.70%\n",
      "Epoch 5, Batch 10, Loss: 20.7367, Accuracy: 57.03%\n",
      "Epoch 5, Batch 20, Loss: 21.2262, Accuracy: 51.56%\n",
      "Epoch 5, Batch 30, Loss: 22.7694, Accuracy: 45.31%\n",
      "Epoch 5, Batch 40, Loss: 19.9981, Accuracy: 57.81%\n",
      "Epoch 5, Batch 50, Loss: 20.9301, Accuracy: 56.25%\n",
      "Epoch 5, Batch 60, Loss: 22.1492, Accuracy: 51.56%\n",
      "Epoch 5, Batch 70, Loss: 20.5638, Accuracy: 50.78%\n",
      "Epoch 5, Batch 80, Loss: 19.4762, Accuracy: 50.00%\n",
      "Epoch 5, Batch 90, Loss: 19.7772, Accuracy: 60.16%\n",
      "Epoch 5, Batch 100, Loss: 22.6482, Accuracy: 57.03%\n",
      "Epoch 5, Batch 110, Loss: 22.2566, Accuracy: 53.12%\n",
      "Epoch 5, Batch 120, Loss: 21.8262, Accuracy: 52.34%\n",
      "Epoch 5, Batch 130, Loss: 17.2352, Accuracy: 63.28%\n",
      "Epoch 5, Batch 140, Loss: 18.3756, Accuracy: 60.94%\n",
      "Epoch 5, Batch 150, Loss: 20.9717, Accuracy: 57.03%\n",
      "Epoch 5, Batch 160, Loss: 19.8606, Accuracy: 51.56%\n",
      "Epoch 5, Batch 170, Loss: 21.8285, Accuracy: 53.12%\n",
      "Epoch 5, Batch 180, Loss: 21.6704, Accuracy: 56.25%\n",
      "Epoch 5, Batch 190, Loss: 19.6162, Accuracy: 58.59%\n",
      "Epoch 5, Batch 200, Loss: 18.7469, Accuracy: 59.38%\n",
      "Epoch 5, Batch 210, Loss: 20.0631, Accuracy: 60.16%\n",
      "Epoch 5, Batch 220, Loss: 20.6747, Accuracy: 56.25%\n",
      "Epoch 5, Batch 230, Loss: 19.4223, Accuracy: 57.03%\n",
      "Epoch 5, Batch 240, Loss: 20.2016, Accuracy: 52.34%\n",
      "Epoch 5, Batch 250, Loss: 18.6892, Accuracy: 57.03%\n",
      "Epoch 5, Batch 260, Loss: 20.3295, Accuracy: 52.34%\n",
      "Epoch 5, Batch 270, Loss: 21.4627, Accuracy: 57.81%\n",
      "Epoch 5, Batch 280, Loss: 19.8448, Accuracy: 59.38%\n",
      "Epoch 5, Batch 290, Loss: 21.2597, Accuracy: 60.94%\n",
      "Epoch 5, Batch 300, Loss: 18.5566, Accuracy: 65.62%\n",
      "Epoch 5, Batch 310, Loss: 19.6269, Accuracy: 54.69%\n",
      "Epoch 5, Batch 320, Loss: 20.3990, Accuracy: 57.81%\n",
      "Epoch 5, Batch 330, Loss: 18.8136, Accuracy: 60.94%\n",
      "Epoch 5, Batch 340, Loss: 19.2340, Accuracy: 63.28%\n",
      "Epoch 5, Batch 350, Loss: 19.7373, Accuracy: 58.59%\n",
      "Epoch 5, Batch 360, Loss: 17.6610, Accuracy: 58.59%\n",
      "Epoch 5, Batch 370, Loss: 22.3902, Accuracy: 54.69%\n",
      "Epoch 5, Batch 380, Loss: 18.2470, Accuracy: 62.50%\n",
      "Epoch 5, Batch 390, Loss: 18.9166, Accuracy: 57.03%\n",
      "Epoch 5, Batch 400, Loss: 22.8113, Accuracy: 51.56%\n",
      "Epoch 5, Batch 410, Loss: 22.3626, Accuracy: 59.38%\n",
      "Epoch 5, Batch 420, Loss: 17.8092, Accuracy: 62.50%\n",
      "Epoch 5, Batch 430, Loss: 16.9205, Accuracy: 63.28%\n",
      "Epoch 5, Batch 440, Loss: 23.1779, Accuracy: 59.38%\n",
      "Epoch 5, Batch 450, Loss: 20.8596, Accuracy: 58.59%\n",
      "Epoch 5, Batch 460, Loss: 18.3479, Accuracy: 61.72%\n",
      "Epoch 5/5, Loss: 9521.3959, Accuracy: 57.30%\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    Conv((28,28,1), (relu, relu_prime), (3,3), 6, init=\"he\"),\n",
    "    MaxPool((26,26,6), (2,2), stride=2),\n",
    "    Conv((13,13,6), (relu, relu_prime), (3,3), 6, init=\"he\"),\n",
    "    MaxPool((11,11,6), (2,2), stride=2),\n",
    "    Flatten(),\n",
    "    Dense(150, 100, (relu, relu_prime), init=\"he\"),\n",
    "    Dense(100, 80, (relu, relu_prime), init=\"he\"),\n",
    "    Dense(80, 10, (softmax, softmax_prime), init=\"he\")\n",
    "]\n",
    "\n",
    "nn = NeuralNetwork(layers, cost=(cross_entropy, cross_entropy_prime))\n",
    "\n",
    "nn.train(x_train, y_train, epochs=5, batch_size=128, eta=0.5, results=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
